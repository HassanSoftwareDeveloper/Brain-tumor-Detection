{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCBwwhrKcDA3"
      },
      "outputs": [],
      "source": [
        "# %% [markdown]\n",
        "# # LongiTumorSense Model Training\n",
        "# **Training on MU-Glioma-Post Dataset**\n",
        "# - Segmentation: nnUNet\n",
        "# - Classification: 3D DenseNet\n",
        "# - Survival: CoxPH Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4J-3uM15IHO3"
      },
      "outputs": [],
      "source": [
        "!pip install monai torch torchvision nnunet pyradiomics lifelines pydicom nibabel wandb -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qiq-ZeBbCbIA"
      },
      "outputs": [],
      "source": [
        "import nibabel as nib\n",
        "import numpy  as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import os\n",
        "import monai\n",
        "from monai.data import Dataset ,DataLoader\n",
        "from monai.transforms import ( Compose , LoadImaged , EnsureChannelFirstd, ScaleIntensityd,RandRotated,RandFlipd,RandZoomd,ToTensord)\n",
        "from monai.networks.nets import DenseNet121,Unet\n",
        "from monai.metrics import DiceMetric\n",
        "from monai.losses import DiceLoss, FocalLoss\n",
        "import wandb\n",
        "import pandas as pd\n",
        "from lifelines import CoxPHFitter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTTpC4VwC0Do",
        "outputId": "c4c6b560-18c9-41df-b757-87d89ff4e5d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device.\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "\n",
        "print(f\"Using {device} device.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhwEtsIvcNmo",
        "outputId": "3a54157e-0a47-463e-9f85-33b51b5df215"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Clearner and saved into the drive**"
      ],
      "metadata": {
        "id": "Ri04EaJBM6Oh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8OKuE6ycWJn",
        "outputId": "9f1461b1-92f9-416a-ef60-1817652f4915"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw dataset path: /content/drive/My Drive/MU-Glioma-Post\n",
            "nnU-Net dataset path: /content/drive/My Drive/clean_data\n",
            "Found 4 cases already processed.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "\n",
        "raw_root = \"/content/drive/My Drive/MU-Glioma-Post\"\n",
        "\n",
        "output_root=\"/content/drive/My Drive/clean_data\"\n",
        "# output_root = \"/content/nnUNet_raw_data_base/Task001_MU-Glioma-Post\"\n",
        "\n",
        "\n",
        "imagesTr = os.path.join(output_root, \"imagesTr\")\n",
        "labelsTr = os.path.join(output_root, \"labelsTr\")\n",
        "os.makedirs(os.path.join(output_root, \"imagesTs\"), exist_ok=True)\n",
        "os.makedirs(imagesTr, exist_ok=True)\n",
        "os.makedirs(labelsTr, exist_ok=True)\n",
        "\n",
        "print(\"Raw dataset path:\", raw_root)\n",
        "print(\"nnU-Net dataset path:\", output_root)\n",
        "\n",
        "\n",
        "progress_file = os.path.join(output_root, \"converted_cases.txt\")\n",
        "\n",
        "if os.path.exists(progress_file):\n",
        "    with open(progress_file, \"r\") as f:\n",
        "        converted_cases = set(line.strip() for line in f)\n",
        "else:\n",
        "    converted_cases = set()\n",
        "print(f\"Found {len(converted_cases)} cases already processed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "qqpZTvVpf7mf"
      },
      "outputs": [],
      "source": [
        "def is_nifti(fname):\n",
        "  return fname.endswith(\".nii\") or fname.endswith(\".nii.gz\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "WYZeKxppgCo3"
      },
      "outputs": [],
      "source": [
        "mod_priority=[\n",
        "      't1c','t1gd','t1ce',  # contrast-enhanced T1 variants\n",
        "    't1n','t1',           # native T1\n",
        "    'flair','t2f','t2flair','t2w','t2' # T2 /flair variants\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "_Iev103XgQPf"
      },
      "outputs": [],
      "source": [
        "def file_priority(fname):\n",
        "  lf=fname.lower()\n",
        "  for i,k in enumerate(mod_priority):\n",
        "    if k in lf:\n",
        "      return i\n",
        "  return len(mod_priority) + hash(lf) % 1000"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import os\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "canonical_modalities = None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "skipped = []\n",
        "new_cases_count = 0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "total_timepoints = sum(\n",
        "    1 for p in sorted(os.listdir(raw_root))\n",
        "    if os.path.isdir(os.path.join(raw_root, p))\n",
        "    for tp in sorted(os.listdir(os.path.join(raw_root, p)))\n",
        "    if os.path.isdir(os.path.join(raw_root, p, tp))\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "with tqdm(total=total_timepoints, desc=\"Processing cases\") as pbar:\n",
        "    for patient_id in sorted(os.listdir(raw_root)):\n",
        "        patient_path = os.path.join(raw_root, patient_id)\n",
        "        if not os.path.isdir(patient_path):\n",
        "            pbar.update(1)\n",
        "            continue\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        for tp in sorted(os.listdir(patient_path)):\n",
        "            tp_path = os.path.join(patient_path, tp)\n",
        "            if not os.path.isdir(tp_path):\n",
        "                pbar.update(1)\n",
        "                continue\n",
        "\n",
        "\n",
        "\n",
        "            tp_clean = re.sub(r\"\\s+\", \"_\", tp)\n",
        "            tp_clean = re.sub(r\"[^A-Za-z0-9_-]\", \"_\", tp_clean)\n",
        "            case_id = f\"{patient_id}_{tp_clean}\"\n",
        "\n",
        "\n",
        "\n",
        "            if case_id in converted_cases:\n",
        "                pbar.update(1)\n",
        "                continue\n",
        "\n",
        "\n",
        "            files = [f for f in os.listdir(tp_path) if is_nifti(f)]\n",
        "            if not files:\n",
        "                skipped.append((patient_id, tp, \"no nifti files\"))\n",
        "                pbar.update(1)\n",
        "                continue\n",
        "\n",
        "\n",
        "\n",
        "            label_candidates = [f for f in files if any(x in f.lower() for x in [\"mask\", \"tumor\", \"seg\", \"label\"])]\n",
        "            if len(label_candidates) == 0:\n",
        "                skipped.append((patient_id, tp, \"no label found\"))\n",
        "                pbar.update(1)\n",
        "                continue\n",
        "\n",
        "\n",
        "            label_file = label_candidates[0]\n",
        "\n",
        "\n",
        "\n",
        "            image_files = [f for f in files if f != label_file]\n",
        "            if len(image_files) == 0:\n",
        "                skipped.append((patient_id, tp, \"no image files\"))\n",
        "                pbar.update(1)\n",
        "                continue\n",
        "\n",
        "\n",
        "\n",
        "            image_files_sorted = sorted(image_files, key=file_priority)\n",
        "            if canonical_modalities is None:\n",
        "                canonical_modalities = image_files_sorted.copy()\n",
        "                print(\"\\nDetected modality order (from first sample)\")\n",
        "                for idx, nm in enumerate(canonical_modalities):\n",
        "                    print(f\"{idx}: {nm}\")\n",
        "                print(\"If this order is wrong adjust mod_priority list in the script.\")\n",
        "\n",
        "\n",
        "            else:\n",
        "                if len(image_files_sorted) != len(canonical_modalities):\n",
        "                    skipped.append(\n",
        "                        (patient_id, tp, f\"modality count mismatch {len(image_files_sorted)} vs {len(canonical_modalities)}\")\n",
        "                    )\n",
        "                    pbar.update(1)\n",
        "                    continue\n",
        "\n",
        "\n",
        "\n",
        "            for i, fname in enumerate(image_files_sorted):\n",
        "                src = os.path.join(tp_path, fname)\n",
        "                destination = os.path.join(imagesTr, f\"{case_id}_{i:04d}.nii.gz\")\n",
        "                shutil.copy(src, destination)\n",
        "\n",
        "            shutil.copy2(os.path.join(tp_path, label_file), os.path.join(labelsTr, f\"{case_id}.nii.gz\"))\n",
        "\n",
        "\n",
        "\n",
        "            converted_cases.add(case_id)\n",
        "            with open(progress_file, \"a\") as f:\n",
        "                f.write(case_id + \"\\n\")\n",
        "\n",
        "            new_cases_count += 1\n",
        "            pbar.update(1)\n",
        "\n",
        "print(f\"\\nConversion finished. {len(converted_cases)} total cases processed so far.\")\n",
        "if skipped:\n",
        "    print(f\"{len(skipped)} timepoints skipped (see sample):\")\n",
        "    for s in skipped[:10]:\n",
        "        print(\" \", s)\n",
        "\n",
        "print(f\"imagesTr files: {len(os.listdir(imagesTr))}, labelsTr files: {len(os.listdir(labelsTr))}\")\n",
        "print(f\"Newly processed this run: {new_cases_count}\")"
      ],
      "metadata": {
        "id": "0n_o0C16-mDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Get clean data from drive into local colab for further processing**"
      ],
      "metadata": {
        "id": "YyI_fuYGDqA5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "\n",
        "drive_clean_path = \"/content/drive/MyDrive/clean_data\"\n",
        "local_clean_path = \"/content/clean_data_local\"\n",
        "\n",
        "os.makedirs(local_clean_path,exist_ok=True)\n",
        "\n",
        "\n",
        "all_files=[]\n",
        "\n",
        "for root,dirs,files in os.walk(drive_clean_path):\n",
        "    for file in files:\n",
        "      source_file=os.path.join(root,file)\n",
        "      relative_path=os.path.relpath(source_file,drive_clean_path)\n",
        "      destination_file=os.path.join(local_clean_path,relative_path)\n",
        "      all_files.append((source_file, destination_file))\n",
        "\n",
        "\n",
        "remaining_files=[]\n",
        "for source_file,destination_file in all_files:\n",
        "    if os.path.exists(destination_file)and os.path.getsize(destination_file)==os.path.getsize(source_file):\n",
        "       continue\n",
        "    remaining_files.append((source_file,destination_file))\n",
        "\n",
        "for source_file,destination_file in tqdm(remaining_files, desc=\"copying files\", unit=\"files\"):\n",
        "    os.makedirs(os.path.dirname(destination_file),exist_ok=True)\n",
        "    shutil.copy2(source_file , destination_file)\n",
        "\n",
        "print(f\"copy complete!{len(all_files)-len(remaining_files)} files already exists,{len(remaining_files)} new files copied\")\n",
        "print(\" Clean dataset loaded from Drive.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8mxhBhBAW6h",
        "outputId": "8b784486-a5b4-4d4a-cfdc-76225ad49ef0"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "copying files: 100%|██████████| 1/1 [00:00<00:00,  1.28files/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "copy complete!2971 files already exists,1 new files copied\n",
            " Clean dataset loaded from Drive.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This is for checking the length of file for each imageTr and labelTr**"
      ],
      "metadata": {
        "id": "rGGZ6yiKMurh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imagesTr_path=os.path.join(local_clean_path,\"imagesTr\")\n",
        "labelsTr_path=os.path.join(local_clean_path,\"labelsTr\")\n",
        "\n",
        "length_imageTr=len([f for f in os.listdir(imagesTr_path) if os.path.isfile(os.path.join(imagesTr_path,f))])\n",
        "length_labelsTr=len([f for f in os.listdir(labelsTr_path) if os.path.isfile(os.path.join(labelsTr_path,f))])\n",
        "\n",
        "print(f\" imageTr files:{length_imageTr}\")\n",
        "print(f\" labelsTr files:{length_labelsTr}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSbJP58r7tKT",
        "outputId": "b9976b9a-129e-4106-b4b0-272735305bd0"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " imageTr files:2376\n",
            " labelsTr files:594\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiegR9RiCE9l"
      },
      "source": [
        "**After Disconnect:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IGS3nUAZ22D0"
      },
      "outputs": [],
      "source": [
        "!pip install monai torch torchvision nnunet pyradiomics lifelines pydicom nibabel wandb -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxdV-3ovAz3h"
      },
      "source": [
        "**Install a Python package directly from its GitHub source code, not from the normal package store (PyPI).”**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/MIC-DKFZ/nnUNet.git"
      ],
      "metadata": {
        "id": "XyYDlsxDkyHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEzaBMYLCTVI"
      },
      "source": [
        "**This function loads an MRI file, converts it to a NumPy array, and scales all values to between 0 and 1 for easier analysis.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YOcMNyv3YC7"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Mr2v-669CSwg"
      },
      "outputs": [],
      "source": [
        "import nibabel as nib\n",
        "import numpy  as np\n",
        "\n",
        "def load_and_preprocess(patient_path):\n",
        "    img = nib.load(patient_path)\n",
        "    data = img.get_fdata()\n",
        "    data = (data - np.min(data)) / (np.max(data) - np.min(data))\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "QNh_mJI2IVA5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "import shutil\n",
        "from glob import glob\n",
        "\n",
        "\n",
        "os.environ['nnUNet_raw_data_base'] = \"/content/nnUNet_raw_data_base\"\n",
        "os.environ['nnUNet_preprocessed'] = \"/content/nnUNet_preprocessed\"\n",
        "os.environ['RESULTS_FOLDER'] = \"/content/nnUNet_results\"\n",
        "\n",
        "\n",
        "\n",
        "os.makedirs(\"/content/nnUNet_raw_data_base\", exist_ok=True)\n",
        "os.makedirs(\"/content/nnUNet_preprocessed\", exist_ok=True)\n",
        "os.makedirs(\"/content/nnUNet_results\", exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Renames the file into nnuNet naming style**\n",
        "\n",
        "**Copy and rename image files**\n",
        "\n",
        "**Copy and rename label files**\n"
      ],
      "metadata": {
        "id": "RrALGUyCQ8O2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import shutil\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "\n",
        "source_images = \"/content/drive/MyDrive/clean_data/imagesTr\"\n",
        "source_labels = \"/content/drive/MyDrive/clean_data/labelsTr\"\n",
        "\n",
        "destination_imagesTr = \"/content/clean_data_local/imagesTr\"\n",
        "destination_labelsTr = \"/content/clean_data_local/labelsTr\"\n",
        "\n",
        "os.makedirs(destination_imagesTr, exist_ok=True)\n",
        "os.makedirs(destination_labelsTr, exist_ok=True)\n",
        "\n",
        "image_files = glob(os.path.join(source_images, \"*.nii.gz\"))\n",
        "label_files = glob(os.path.join(source_labels, \"*.nii.gz\"))\n",
        "\n",
        "\n",
        "\n",
        "print(f\"Copying {len(image_files)} image files...\")\n",
        "\n",
        "\n",
        "for scan_path in tqdm(image_files, desc=\"Images copied\", unit=\"file\"):\n",
        "    filename = os.path.basename(scan_path)\n",
        "    match = re.match(r\"(PatientID_\\d+)_Timepoint_(\\d+)_(\\d{4})\\.nii\\.gz\", filename)\n",
        "    if match:\n",
        "        patient_id, timepoint, modality_idx = match.groups()\n",
        "        case_id = f\"{patient_id}_Timepoint_{timepoint}\"\n",
        "        destination_path = os.path.join(destination_imagesTr, f\"{case_id}_{modality_idx}.nii.gz\")\n",
        "        if scan_path != destination_path:\n",
        "            shutil.copy(scan_path, destination_path)\n",
        "\n",
        "print(f\"Copying {len(label_files)} label files...\")\n",
        "\n",
        "\n",
        "for label_path in tqdm(label_files, desc=\"Labels copied\", unit=\"file\"):\n",
        "    filename = os.path.basename(label_path)\n",
        "    match = re.match(r\"(PatientID_\\d+)_Timepoint_(\\d+)\\.nii\\.gz\", filename)\n",
        "    if match:\n",
        "        patient_id, timepoint = match.groups()\n",
        "        case_id = f\"{patient_id}_Timepoint_{timepoint}\"\n",
        "        dst_path = os.path.join(destination_labelsTr, f\"{case_id}.nii.gz\")\n",
        "        if label_path != dst_path:\n",
        "            shutil.copy(label_path, dst_path)\n",
        "\n",
        "print(f\"Total copied {len(os.listdir(destination_imagesTr))} scans to {destination_imagesTr}\")\n",
        "print(f\"Total copied {len(os.listdir(destination_labelsTr))} labels to {destination_labelsTr}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ExmO0wKU5I1",
        "outputId": "3360ce15-26e3-4373-fb82-58fc79f7484c"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying 2376 image files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Images copied: 100%|██████████| 2376/2376 [07:44<00:00,  5.12file/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying 594 label files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Labels copied: 100%|██████████| 594/594 [03:10<00:00,  3.12file/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total copied 2376 scans to /content/clean_data_local/imagesTr\n",
            "Total copied 594 labels to /content/clean_data_local/labelsTr\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This code creates a dataset.json file that describes your medical imaging dataset for nnU-Net.**"
      ],
      "metadata": {
        "id": "2SqZer9eXjVk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeNuFCGIdUuC",
        "outputId": "2ffaa2fb-0a2c-4a9e-b005-c566a8ae6708"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset.json created at: /content/clean_data_local/dataset.json\n",
            "{\n",
            "    \"name\": \"MU-Glioma-Post\",\n",
            "    \"description\": \"Post-operative glioma segmentation\",\n",
            "    \"reference\": \"Your reference here\",\n",
            "    \"licence\": \"Your license here\",\n",
            "    \"release\": \"1.0\",\n",
            "    \"modality\": {\n",
            "        \"0\": \"MRI_modality_0\",\n",
            "        \"1\": \"MRI_modality_1\",\n",
            "        \"2\": \"MRI_modality_2\",\n",
            "        \"3\": \"MRI_modality_3\"\n",
            "    },\n",
            "    \"labels\": {\n",
            "        \"0\": \"background\",\n",
            "        \"1\": \"tumor\"\n",
            "    },\n",
            "    \"numTraining\": 594,\n",
            "    \"file_ending\": \".nii.gz\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "\n",
        "\n",
        "output_root = \"/content/clean_data_local\"\n",
        "imagesTr_path = os.path.join(output_root, \"imagesTr\")\n",
        "labelsTr_path = os.path.join(output_root, \"labelsTr\")\n",
        "\n",
        "\n",
        "num_cases = len([f for f in os.listdir(labelsTr_path) if f.endswith(\".nii.gz\")])\n",
        "\n",
        "\n",
        "first_case_files = sorted([f for f in os.listdir(imagesTr_path) if f.endswith(\".nii.gz\")])\n",
        "modality_count = len(set([re.search(r'_(\\d{4})\\.nii\\.gz$', f).group(1) for f in first_case_files]))\n",
        "\n",
        "\n",
        "dataset_json = {\n",
        "    \"name\": \"MU-Glioma-Post\",\n",
        "    \"description\": \"Post-operative glioma segmentation\",\n",
        "    \"reference\": \"Your reference here\",\n",
        "    \"licence\": \"Your license here\",\n",
        "    \"release\": \"1.0\",\n",
        "    \"modality\": {str(i): f\"MRI_modality_{i}\" for i in range(modality_count)},\n",
        "    \"labels\": {\n",
        "        \"0\": \"background\",\n",
        "        \"1\": \"tumor\"\n",
        "    },\n",
        "    \"numTraining\": num_cases,\n",
        "    \"file_ending\": \".nii.gz\"\n",
        "}\n",
        "\n",
        "\n",
        "with open(os.path.join(output_root, \"dataset.json\"), 'w') as f:\n",
        "    json.dump(dataset_json, f, indent=4)\n",
        "\n",
        "print(f\"dataset.json created at: {os.path.join(output_root, 'dataset.json')}\")\n",
        "print(json.dumps(dataset_json, indent=4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkjWjNWHBBdW"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n",
        "# Print your default W&B username (entity)\n",
        "print(\"Your W&B username:\", wandb.Api().default_entity)\n",
        "\n",
        "# Alternative: Check after login\n",
        "wandb.login()\n",
        "print(\"Logged in as:\", wandb.Api().default_entity)"
      ],
      "metadata": {
        "id": "VRxzFScnoqqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "4HFLOc_RCO_e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 688
        },
        "outputId": "83636e84-ee1d-4326-ff07-eacb2a3bf02b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnuml-f22-44939\u001b[0m (\u001b[33mnuml-f21-35629-numl\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250810_154137-p8ne2fsa</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/numl-f21-35629-numl/LongiTumorSense/runs/p8ne2fsa' target=\"_blank\">charmed-dust-5</a></strong> to <a href='https://wandb.ai/numl-f21-35629-numl/LongiTumorSense' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/numl-f21-35629-numl/LongiTumorSense' target=\"_blank\">https://wandb.ai/numl-f21-35629-numl/LongiTumorSense</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/numl-f21-35629-numl/LongiTumorSense/runs/p8ne2fsa' target=\"_blank\">https://wandb.ai/numl-f21-35629-numl/LongiTumorSense/runs/p8ne2fsa</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/numl-f21-35629-numl/LongiTumorSense/runs/p8ne2fsa?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7e1c3a1d8950>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "wandb.init(project=\"LongiTumorSense\",entity=\"numl-f21-35629-numl\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def prepare_dataset(imagesTr, labelsTr, test_size=0.2):\n",
        "\n",
        "    image_files = [f for f in os.listdir(imagesTr) if f.endswith(\".nii.gz\")]\n",
        "\n",
        "\n",
        "    case_ids = sorted(list(set(\"_\".join(f.split(\"_\")[:-1]) for f in image_files)))\n",
        "\n",
        "    print(f\"Found {len(case_ids)} unique cases.\")\n",
        "\n",
        "    # Split into train and test\n",
        "    train_cases, test_cases = train_test_split(case_ids, test_size=test_size, random_state=42)\n",
        "\n",
        "    train_files, test_files = [], []\n",
        "    missing_labels = []\n",
        "\n",
        "    def build_file_list(cases):\n",
        "        file_list = []\n",
        "        for case_id in cases:\n",
        "            img_path = os.path.join(imagesTr, f\"{case_id}_0000.nii.gz\")\n",
        "            label_path = os.path.join(labelsTr, f\"{case_id}.nii.gz\")\n",
        "\n",
        "            if not os.path.exists(label_path):\n",
        "                missing_labels.append(case_id)\n",
        "                continue\n",
        "\n",
        "            file_list.append({\n",
        "                \"image\": img_path,\n",
        "                \"label\": label_path,\n",
        "                \"name\": case_id\n",
        "            })\n",
        "        return file_list\n",
        "\n",
        "    train_files = build_file_list(train_cases)\n",
        "    test_files = build_file_list(test_cases)\n",
        "\n",
        "    print(f\"Length of training dataset: {len(train_files)}\")\n",
        "    print(f\"Length of validation dataset: {len(test_files)}\")\n",
        "\n",
        "    if missing_labels:\n",
        "        print(f\"Missing labels for {len(missing_labels)} cases: {missing_labels[:10]}{'...' if len(missing_labels) > 10 else ''}\")\n",
        "\n",
        "    return train_files, test_files\n",
        "\n"
      ],
      "metadata": {
        "id": "tNhBNBQTuQCH"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_files, test_files = prepare_dataset(\n",
        "    \"/content/clean_data_local/imagesTr\",\n",
        "    \"/content/clean_data_local/labelsTr\"\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bd4Qh8G7Z7Us",
        "outputId": "c0e16e75-7451-47a6-9958-b0f8174f0c28"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 594 unique cases.\n",
            "Length of training dataset: 475\n",
            "Length of validation dataset: 119\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "adaf53cFCo21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3112077-66a8-4710-be67-ea52b38fb14d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of training dataset: 475\n",
            "Length of validation dataset: 119\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"Length of training dataset:\", len(train_files))\n",
        "print(\"Length of validation dataset:\", len(test_files))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bwp0vLhAMGLZ"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "2M4QzyL6WNaf"
      },
      "outputs": [],
      "source": [
        "train_trainsforms=Compose([\n",
        "    LoadImaged(keys=[\"image\",\"label\"]),\n",
        "    EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
        "    ScaleIntensityd(keys=[\"image\"]),\n",
        "    RandRotated(keys=[\"image\", \"label\"],range_x=0.3,prob=0.5),\n",
        "    RandFlipd(keys=[\"image\", \"label\"],prob=0.5),\n",
        "    RandZoomd(keys=[\"image\", \"label\"],min_zoom=0.9,max_zoom=1.1,prob=0.5),\n",
        "    ToTensord(keys=[\"image\", \"label\"])\n",
        "])\n",
        "\n",
        "\n",
        "val_transforms = Compose([\n",
        "    LoadImaged(keys=[\"image\", \"label\"]),\n",
        "    EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
        "    ScaleIntensityd(keys=[\"image\"]),\n",
        "    ToTensord(keys=[\"image\", \"label\"])\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "fuEVd74Ei12y"
      },
      "outputs": [],
      "source": [
        "train_ds=Dataset(data=train_files,transform=train_trainsforms)\n",
        "val_ds=Dataset(data=test_files,transform=val_transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ws6BPrPaaoe",
        "outputId": "c2c82c46-0c74-4664-c484-5b7060d992aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'monai.data.dataloader.DataLoader'>\n",
            "<class 'monai.data.dataloader.DataLoader'>\n"
          ]
        }
      ],
      "source": [
        "train_loader=DataLoader(train_ds,batch_size=4,shuffle=True)\n",
        "val_loader=DataLoader(val_ds,batch_size=2,shuffle=False)\n",
        "print(type(train_loader))\n",
        "print(type(val_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "vvIfZbq4BsOX"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4zGBucN1xZ_"
      },
      "source": [
        "\n",
        "**# Convert dataset to nnUNet format**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2S3cmHha2lYv"
      },
      "outputs": [],
      "source": [
        "!nnUNet_convert_decathlon_task -i /content/nnUNet_raw_data_base/Task001_MU-Glioma-Post -p  1 -output_task_id 001"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}