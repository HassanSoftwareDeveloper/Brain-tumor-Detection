{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "VCBwwhrKcDA3"
      },
      "outputs": [],
      "source": [
        "# %% [markdown]\n",
        "# # LongiTumorSense Model Training\n",
        "# **Training on MU-Glioma-Post Dataset**\n",
        "# - Segmentation: nnUNet\n",
        "# - Classification: 3D DenseNet\n",
        "# - Survival: CoxPH Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhwEtsIvcNmo",
        "outputId": "cc22b733-ca8b-4f32-8da8-d5574e74da89"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "\n",
        "raw_root = \"/content/drive/My Drive/MU-Glioma-Post\"\n",
        "\n",
        "\n",
        "output_root = \"/content/nnUNet_raw_data_base/Task001_MU-Glioma-Post\"\n",
        "\n",
        "\n",
        "imagesTr = os.path.join(output_root, \"imagesTr\")\n",
        "labelsTr = os.path.join(output_root, \"labelsTr\")\n",
        "os.makedirs(imagesTr, exist_ok=True)\n",
        "os.makedirs(labelsTr, exist_ok=True)\n",
        "\n",
        "print(\"Raw dataset path:\", raw_root)\n",
        "print(\"nnU-Net dataset path:\", output_root)\n",
        "\n",
        "\n",
        "progress_file = os.path.join(output_root, \"converted_cases.txt\")\n",
        "\n",
        "if os.path.exists(progress_file):\n",
        "    with open(progress_file, \"r\") as f:\n",
        "        converted_cases = set(line.strip() for line in f)\n",
        "else:\n",
        "    converted_cases = set()\n",
        "print(f\"Found {len(converted_cases)} cases already processed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8OKuE6ycWJn",
        "outputId": "737c204e-15e3-4097-b880-9b2c1d72b3c2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw dataset path: /content/drive/My Drive/MU-Glioma-Post\n",
            "nnU-Net dataset path: /content/nnUNet_raw_data_base/Task001_MU-Glioma-Post\n",
            "Found 0 cases already processed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def is_nifti(fname):\n",
        "  return fname.endswith(\".nii\") or fname.endswith(\".nii.gz\")"
      ],
      "metadata": {
        "id": "qqpZTvVpf7mf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mod_priority=[\n",
        "      't1c','t1gd','t1ce',  # contrast-enhanced T1 variants\n",
        "    't1n','t1',           # native T1\n",
        "    'flair','t2f','t2flair','t2w','t2' # T2 /flair variants\n",
        "]"
      ],
      "metadata": {
        "id": "WYZeKxppgCo3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def file_priority(fname):\n",
        "  lf=fname.lower()\n",
        "  for i,k in enumerate(mod_priority):\n",
        "    if k in lf:\n",
        "      return i\n",
        "  return len(mod_priority) + hash(lf) % 1000"
      ],
      "metadata": {
        "id": "_Iev103XgQPf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "skipped = []\n",
        "canonical_modalities = None\n",
        "for patient_id in sorted(os.listdir(raw_root)):\n",
        "    patient_path = os.path.join(raw_root, patient_id)\n",
        "    if not os.path.isdir(patient_path):\n",
        "        continue\n",
        "\n",
        "    for tp in sorted(os.listdir(patient_path)):\n",
        "        tp_path = os.path.join(patient_path, tp)\n",
        "        if not os.path.isdir(tp_path):\n",
        "            continue\n",
        "\n",
        "        tp_clean = re.sub(r\"\\s+\", \"_\", tp)\n",
        "        tp_clean = re.sub(r\"[^A-Za-z0-9_-]\", \"_\", tp_clean)\n",
        "        case_id = f\"{patient_id}_{tp_clean}\"\n",
        "\n",
        "\n",
        "        if case_id in converted_cases:\n",
        "            print(f\"Skipping {case_id}, already processed.\")\n",
        "            continue\n",
        "\n",
        "        files = [f for f in os.listdir(tp_path) if is_nifti(f)]\n",
        "        if not files:\n",
        "            skipped.append((patient_id, tp, \"no nifti files\"))\n",
        "            continue\n",
        "\n",
        "        label_candidates = [\n",
        "            f for f in files if any(x in f.lower() for x in [\"mask\", \"tumor\", \"seg\", \"label\"])\n",
        "        ]\n",
        "        if len(label_candidates) == 0:\n",
        "            skipped.append((patient_id, tp, \"no label found\"))\n",
        "            continue\n",
        "\n",
        "        label_file = label_candidates[0]\n",
        "        image_files = [f for f in files if f != label_file]\n",
        "        if len(image_files) == 0:\n",
        "            skipped.append((patient_id, tp, \"no image files\"))\n",
        "            continue\n",
        "\n",
        "        image_files_sorted = sorted(image_files, key=file_priority)\n",
        "        if canonical_modalities is None:\n",
        "            canonical_modalities = image_files_sorted.copy()\n",
        "            print(\"Detected modality order (from first sample)\")\n",
        "            for idx, nm in enumerate(canonical_modalities):\n",
        "                print(f\"{idx}:{nm}\")\n",
        "            print(\"If this order is wrong adjust mod_priority list in the script.\")\n",
        "        else:\n",
        "            if len(image_files_sorted) != len(canonical_modalities):\n",
        "                skipped.append(\n",
        "                    (patient_id, tp, f\"modality count mismatch: {len(image_files_sorted)} vs {len(canonical_modalities)}\")\n",
        "                )\n",
        "                continue\n",
        "\n",
        "        for i, fname in enumerate(image_files_sorted):\n",
        "            src = os.path.join(tp_path, fname)\n",
        "            destination = os.path.join(imagesTr, f\"{case_id}_{i:04d}.nii.gz\")\n",
        "            shutil.copy(src, destination)\n",
        "\n",
        "        shutil.copy2(\n",
        "            os.path.join(tp_path, label_file),\n",
        "            os.path.join(labelsTr, f\"{case_id}.nii.gz\")\n",
        "        )\n",
        "\n",
        "\n",
        "        converted_cases.add(case_id)\n",
        "        with open(progress_file, \"a\") as f:\n",
        "            f.write(case_id + \"\\n\")\n",
        "\n",
        "print(f\"\\nConversion finished. {len(converted_cases)} total cases processed so far.\")\n",
        "\n",
        "if skipped:\n",
        "    print(f\"{len(skipped)} timepoints skipped (see sample):\")\n",
        "    for s in skipped[:10]:\n",
        "        print(\" \", s)\n",
        "\n",
        "print(\n",
        "    f\"imagesTr files: {len(os.listdir(imagesTr))}, labelsTr files: {len(os.listdir(labelsTr))}\"\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONwDTyp3QkCi",
        "outputId": "c578a936-0c18-4bc8-8716-aaa5b5c81f3d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected modality order (from first sample)\n",
            "0:PatientID_0003_Timepoint_1_brain_t1c.nii.gz\n",
            "1:PatientID_0003_Timepoint_1_brain_t1n.nii.gz\n",
            "2:PatientID_0003_Timepoint_1_brain_t2f.nii.gz\n",
            "3:PatientID_0003_Timepoint_1_brain_t2w.nii.gz\n",
            "If this order is wrong adjust mod_priority list in the script.\n",
            "\n",
            "Conversion finished. 594 total cases processed so far.\n",
            "2 timepoints skipped (see sample):\n",
            "  ('PatientID_0187', 'Timepoint_3', 'no label found')\n",
            "  ('PatientID_0191', 'Timepoint_1', 'no label found')\n",
            "imagesTr files: 2376, labelsTr files: 594\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDc7lbhhhf50",
        "outputId": "6c9f0714-b4bd-4bc1-c59a-94017f8c6d38"
      },
      "source": [
        "import re\n",
        "import os\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "canonical_modalities = None\n",
        "\n",
        "skipped = []\n",
        "new_cases_count = 0\n",
        "\n",
        "\n",
        "total_timepoints = sum(\n",
        "    1 for p in sorted(os.listdir(raw_root))\n",
        "    if os.path.isdir(os.path.join(raw_root, p))\n",
        "    for tp in sorted(os.listdir(os.path.join(raw_root, p)))\n",
        "    if os.path.isdir(os.path.join(raw_root, p, tp))\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "with tqdm(total=total_timepoints, desc=\"Processing cases\") as pbar:\n",
        "    for patient_id in sorted(os.listdir(raw_root)):\n",
        "        patient_path = os.path.join(raw_root, patient_id)\n",
        "        if not os.path.isdir(patient_path):\n",
        "            pbar.update(1)\n",
        "            continue\n",
        "\n",
        "\n",
        "\n",
        "        for tp in sorted(os.listdir(patient_path)):\n",
        "            tp_path = os.path.join(patient_path, tp)\n",
        "            if not os.path.isdir(tp_path):\n",
        "                pbar.update(1)\n",
        "                continue\n",
        "\n",
        "\n",
        "            tp_clean = re.sub(r\"\\s+\", \"_\", tp)\n",
        "            tp_clean = re.sub(r\"[^A-Za-z0-9_-]\", \"_\", tp_clean)\n",
        "            case_id = f\"{patient_id}_{tp_clean}\"\n",
        "\n",
        "\n",
        "            if case_id in converted_cases:\n",
        "                pbar.update(1)\n",
        "                continue\n",
        "\n",
        "            files = [f for f in os.listdir(tp_path) if is_nifti(f)]\n",
        "            if not files:\n",
        "                skipped.append((patient_id, tp, \"no nifti files\"))\n",
        "                pbar.update(1)\n",
        "                continue\n",
        "\n",
        "            label_candidates = [f for f in files if any(x in f.lower() for x in [\"mask\", \"tumor\", \"seg\", \"label\"])]\n",
        "            if len(label_candidates) == 0:\n",
        "                skipped.append((patient_id, tp, \"no label found\"))\n",
        "                pbar.update(1)\n",
        "                continue\n",
        "\n",
        "\n",
        "            label_file = label_candidates[0]\n",
        "\n",
        "            image_files = [f for f in files if f != label_file]\n",
        "            if len(image_files) == 0:\n",
        "                skipped.append((patient_id, tp, \"no image files\"))\n",
        "                pbar.update(1)\n",
        "                continue\n",
        "\n",
        "            image_files_sorted = sorted(image_files, key=file_priority)\n",
        "            if canonical_modalities is None:\n",
        "                canonical_modalities = image_files_sorted.copy()\n",
        "                print(\"\\nDetected modality order (from first sample)\")\n",
        "                for idx, nm in enumerate(canonical_modalities):\n",
        "                    print(f\"{idx}: {nm}\")\n",
        "                print(\"If this order is wrong adjust mod_priority list in the script.\")\n",
        "\n",
        "            else:\n",
        "                if len(image_files_sorted) != len(canonical_modalities):\n",
        "                    skipped.append(\n",
        "                        (patient_id, tp, f\"modality count mismatch {len(image_files_sorted)} vs {len(canonical_modalities)}\")\n",
        "                    )\n",
        "                    pbar.update(1)\n",
        "                    continue\n",
        "\n",
        "\n",
        "            for i, fname in enumerate(image_files_sorted):\n",
        "                src = os.path.join(tp_path, fname)\n",
        "                destination = os.path.join(imagesTr, f\"{case_id}_{i:04d}.nii.gz\")\n",
        "                shutil.copy(src, destination)\n",
        "\n",
        "            shutil.copy2(os.path.join(tp_path, label_file), os.path.join(labelsTr, f\"{case_id}.nii.gz\"))\n",
        "\n",
        "            # Save progress immediately\n",
        "            converted_cases.add(case_id)\n",
        "            with open(progress_file, \"a\") as f:\n",
        "                f.write(case_id + \"\\n\")\n",
        "\n",
        "            new_cases_count += 1\n",
        "            pbar.update(1)\n",
        "\n",
        "print(f\"\\nConversion finished. {len(converted_cases)} total cases processed so far.\")\n",
        "if skipped:\n",
        "    print(f\"{len(skipped)} timepoints skipped (see sample):\")\n",
        "    for s in skipped[:10]:\n",
        "        print(\" \", s)\n",
        "\n",
        "print(f\"imagesTr files: {len(os.listdir(imagesTr))}, labelsTr files: {len(os.listdir(labelsTr))}\")\n",
        "print(f\"Newly processed this run: {new_cases_count}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing cases: 100%|██████████| 596/596 [00:00<00:00, 2379.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Conversion finished. 594 total cases processed so far.\n",
            "2 timepoints skipped (see sample):\n",
            "  ('PatientID_0187', 'Timepoint_3', 'no label found')\n",
            "  ('PatientID_0191', 'Timepoint_1', 'no label found')\n",
            "imagesTr files: 2376, labelsTr files: 594\n",
            "Newly processed this run: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "PROGRESS_PATH = \"/content/drive/MyDrive/nnUNet_progress\"\n",
        "os.makedirs(PROGRESS_PATH, exist_ok=True)"
      ],
      "metadata": {
        "id": "FajUzCdF768l"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import os\n",
        "import shutil\n",
        "from glob import glob\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "def process_dataset(raw_root, output_root, task_name=\"Task001_MU-Glioma-Post\"):\n",
        "\n",
        "    PROGRESS_FILE = f\"{PROGRESS_PATH}/{task_name}_progress.json\"\n",
        "    CONVERTED_FILE = f\"{output_root}/converted_cases.txt\"\n",
        "\n",
        "\n",
        "    if os.path.exists(PROGRESS_FILE):\n",
        "        with open(PROGRESS_FILE) as f:\n",
        "            progress = json.load(f)\n",
        "        converted_cases = set(progress['converted_cases'])\n",
        "        skipped = progress['skipped']\n",
        "        print(f\"Resuming with {len(converted_cases)} pre-processed cases\")\n",
        "    else:\n",
        "        converted_cases = set()\n",
        "        skipped = []\n",
        "\n",
        "\n",
        "    imagesTr = os.path.join(output_root, \"imagesTr\")\n",
        "    labelsTr = os.path.join(output_root, \"labelsTr\")\n",
        "    os.makedirs(imagesTr, exist_ok=True)\n",
        "    os.makedirs(labelsTr, exist_ok=True)\n",
        "\n",
        "\n",
        "    def is_case_processed(case_id):\n",
        "        \"\"\"Check if files actually exist\"\"\"\n",
        "        has_images = any(f.startswith(case_id) for f in os.listdir(imagesTr))\n",
        "        has_label = os.path.exists(os.path.join(labelsTr, f\"{case_id}.nii.gz\"))\n",
        "        return has_images and has_label\n",
        "\n",
        "    for patient_id in sorted(os.listdir(raw_root)):\n",
        "        patient_path = os.path.join(raw_root, patient_id)\n",
        "        if not os.path.isdir(patient_path):\n",
        "            continue\n",
        "\n",
        "        for tp in sorted(os.listdir(patient_path)):\n",
        "            tp_path = os.path.join(patient_path, tp)\n",
        "            if not os.path.isdir(tp_path):\n",
        "                continue\n",
        "\n",
        "\n",
        "            tp_clean = re.sub(r\"\\s+\", \"_\", tp)\n",
        "            tp_clean = re.sub(r\"[^A-Za-z0-9_-]\", \"_\", tp_clean)\n",
        "            case_id = f\"{patient_id}_{tp_clean}\"\n",
        "\n",
        "\n",
        "            if case_id in converted_cases and is_case_processed(case_id):\n",
        "                print(f\"✓ {case_id} (already processed)\")\n",
        "                continue\n",
        "\n",
        "\n",
        "\n",
        "            converted_cases.add(case_id)\n",
        "\n",
        "\n",
        "            if len(converted_cases) % 5 == 0:\n",
        "                with open(PROGRESS_FILE, 'w') as f:\n",
        "                    json.dump({\n",
        "                        'converted_cases': list(converted_cases),\n",
        "                        'skipped': skipped,\n",
        "                        'last_update': str(datetime.now())\n",
        "                    }, f)\n",
        "                print(f\"Saved progress after {len(converted_cases)} cases\")\n",
        "\n",
        "\n",
        "    with open(PROGRESS_FILE, 'w') as f:\n",
        "        json.dump({\n",
        "            'converted_cases': list(converted_cases),\n",
        "            'skipped': skipped\n",
        "        }, f)\n",
        "\n",
        "    print(f\"Processing complete. Total cases: {len(converted_cases)}\")"
      ],
      "metadata": {
        "id": "3u76y-_m_zGF"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**After Disconnect:**"
      ],
      "metadata": {
        "id": "HiegR9RiCE9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "process_dataset(\n",
        "    raw_root=\"/content/drive/My Drive/MU-Glioma-Post\",\n",
        "    output_root=\"/content/nnUNet_raw_data_base/Task001_MU-Glioma-Post\"\n",
        ")"
      ],
      "metadata": {
        "id": "-m_J9qNwFC6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install monai torch torchvision nnunet pyradiomics lifelines pydicom nibabel -q"
      ],
      "metadata": {
        "id": "5C3Q9coHFGK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install a Python package directly from its GitHub source code, not from the normal package store (PyPI).”**"
      ],
      "metadata": {
        "id": "KxdV-3ovAz3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/MIC-DKFZ/nnUNet.git"
      ],
      "metadata": {
        "id": "J1elLjKPFRcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nibabel as nib\n",
        "import numpy  as np\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "VtUUjdwACMdh"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This function loads an MRI file, converts it to a NumPy array, and scales all values to between 0 and 1 for easier analysis.**"
      ],
      "metadata": {
        "id": "WEzaBMYLCTVI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nibabel as nib\n",
        "import numpy  as np\n",
        "\n",
        "def load_and_preprocess(patient_path):\n",
        "    img = nib.load(patient_path)\n",
        "    data = img.get_fdata()\n",
        "    data = (data - np.min(data)) / (np.max(data) - np.min(data))\n",
        "    return data"
      ],
      "metadata": {
        "id": "Mr2v-669CSwg"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "/content/Task001_MU-Glioma-Post"
      ],
      "metadata": {
        "id": "jAGmLPqsgGiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "import shutil\n",
        "from glob import glob\n",
        "\n",
        "\n",
        "os.environ['nnUNet_raw_data_base'] = \"/content/nnUNet_raw_data_base\"\n",
        "os.environ['nnUNet_preprocessed'] = \"/content/nnUNet_preprocessed\"\n",
        "os.environ['RESULTS_FOLDER'] = \"/content/nnUNet_results\"\n",
        "\n",
        "os.makedirs(\"/content/nnUNet_raw_data_base\", exist_ok=True)\n",
        "os.makedirs(\"/content/nnUNet_preprocessed\", exist_ok=True)\n",
        "os.makedirs(\"/content/nnUNet_results\", exist_ok=True)\n"
      ],
      "metadata": {
        "id": "QNh_mJI2IVA5"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_root = \"/content/nnUNet_raw_data_base/Task001_MU-Glioma-Post\"\n",
        "imagesTr = os.path.join(output_root, \"imagesTr\")\n",
        "labelsTr = os.path.join(output_root, \"labelsTr\")\n",
        "os.makedirs(imagesTr, exist_ok=True)\n",
        "os.makedirs(labelsTr, exist_ok=True)"
      ],
      "metadata": {
        "id": "lv5mSmfRWQwj"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from glob import glob\n",
        "import re\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "for scan_path in glob(\"/content/Task001_MU-Glioma-Post/imagesTr/*.nii.gz\"):\n",
        "\n",
        "    filename = os.path.basename(scan_path)\n",
        "    match = re.match(r\"(PatientID_\\d+)_Timepoint_(\\d+)_(\\d{4})\\.nii\\.gz\", filename)\n",
        "\n",
        "    if match:\n",
        "        patient_id, timepoint, modality_idx = match.groups()\n",
        "        case_id = f\"{patient_id}_Timepoint_{timepoint}\"\n",
        "\n",
        "\n",
        "        shutil.copy(\n",
        "            scan_path,\n",
        "            os.path.join(imagesTr, f\"{case_id}_{modality_idx}.nii.gz\")\n",
        "        )\n",
        "for label_path in glob(\"/content/Task001_MU-Glioma-Post/labelsTr/*.nii.gz\"):\n",
        "    filename = os.path.basename(label_path)\n",
        "    match = re.match(r\"(PatientID_\\d+)_Timepoint_(\\d+)\\.nii\\.gz\", filename)\n",
        "\n",
        "    if match:\n",
        "        patient_id, timepoint = match.groups()\n",
        "        case_id = f\"{patient_id}_Timepoint_{timepoint}\"\n",
        "\n",
        "        shutil.copy(\n",
        "            label_path,\n",
        "            os.path.join(labelsTr, f\"{case_id}.nii.gz\")\n",
        "        )\n",
        "\n",
        "# 3. Verify copies\n",
        "print(f\"Copied {len(os.listdir(imagesTr))} scans to {imagesTr}\")\n",
        "print(f\"Copied {len(os.listdir(labelsTr))} labels to {labelsTr}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgKlCdlxh-ab",
        "outputId": "d2d52160-5bcb-4276-dbfc-433bfaf8b07e"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copied 2376 scans to /content/nnUNet_raw_data_base/Task001_MU-Glioma-Post/imagesTr\n",
            "Copied 594 labels to /content/nnUNet_raw_data_base/Task001_MU-Glioma-Post/labelsTr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "source_images = \"/content/Task001_MU-Glioma-Post/imagesTr/PatientID_0003_Timepoint_1_0000.nii.gz\"  # Scans\n",
        "source_labels = \"/content/Task001_MU-Glioma-Post/labelsTr/PatientID_0003_Timepoint_1.nii.gz\"  # Labels"
      ],
      "metadata": {
        "id": "NTU9zHmihGlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "\n",
        "# Paths\n",
        "output_root = \"/content/nnUNet_raw_data_base/Task001_MU-Glioma-Post\"\n",
        "imagesTr_path = os.path.join(output_root, \"imagesTr\")\n",
        "labelsTr_path = os.path.join(output_root, \"labelsTr\")\n",
        "\n",
        "# Count training cases\n",
        "num_cases = len([f for f in os.listdir(labelsTr_path) if f.endswith(\".nii.gz\")])\n",
        "\n",
        "# Detect modalities by looking at first case\n",
        "first_case_files = sorted([f for f in os.listdir(imagesTr_path) if f.endswith(\".nii.gz\")])\n",
        "modality_count = len(set([re.search(r'_(\\d{4})\\.nii\\.gz$', f).group(1) for f in first_case_files]))\n",
        "\n",
        "# Build dataset.json dictionary\n",
        "dataset_json = {\n",
        "    \"name\": \"MU-Glioma-Post\",\n",
        "    \"description\": \"Post-operative glioma segmentation\",\n",
        "    \"reference\": \"Your reference here\",\n",
        "    \"licence\": \"Your license here\",\n",
        "    \"release\": \"1.0\",\n",
        "    \"modality\": {str(i): f\"MRI_modality_{i}\" for i in range(modality_count)},\n",
        "    \"labels\": {\n",
        "        \"0\": \"background\",\n",
        "        \"1\": \"tumor\"\n",
        "    },\n",
        "    \"numTraining\": num_cases,\n",
        "    \"file_ending\": \".nii.gz\"\n",
        "}\n",
        "\n",
        "\n",
        "with open(os.path.join(output_root, \"dataset.json\"), 'w') as f:\n",
        "    json.dump(dataset_json, f, indent=4)\n",
        "\n",
        "print(f\"dataset.json created at: {os.path.join(output_root, 'dataset.json')}\")\n",
        "print(json.dumps(dataset_json, indent=4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeNuFCGIdUuC",
        "outputId": "2da4d548-ba0f-4642-d66c-e7a8ea625cd0"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset.json created at: /content/nnUNet_raw_data_base/Task001_MU-Glioma-Post/dataset.json\n",
            "{\n",
            "    \"name\": \"MU-Glioma-Post\",\n",
            "    \"description\": \"Post-operative glioma segmentation\",\n",
            "    \"reference\": \"Your reference here\",\n",
            "    \"licence\": \"Your license here\",\n",
            "    \"release\": \"1.0\",\n",
            "    \"modality\": {\n",
            "        \"0\": \"MRI_modality_0\",\n",
            "        \"1\": \"MRI_modality_1\",\n",
            "        \"2\": \"MRI_modality_2\",\n",
            "        \"3\": \"MRI_modality_3\"\n",
            "    },\n",
            "    \"labels\": {\n",
            "        \"0\": \"background\",\n",
            "        \"1\": \"tumor\"\n",
            "    },\n",
            "    \"numTraining\": 594,\n",
            "    \"file_ending\": \".nii.gz\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from monai.networks.nets import DenseNet"
      ],
      "metadata": {
        "id": "nLt0wUYkbBuy"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3MZztg1PFkdz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}